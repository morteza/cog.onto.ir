---
title: 'An Alarm for a False Alarm'
title_fa:
layout: post_en
date: '2016-09-09'
date_fa:
comments: true
categories: article
link: http://onlinelibrary.wiley.com/doi/10.1111/anae.13300/full
doi: 10.1111/anae.13300
tags:
  - Statistics
  - Hypothesis Testing
  - 'P Value'
  - Scientific Methods
---

> *Choi, S. W., & Lam, D. M. H. (2016). An alarm for a false alarm. Anaesthesia, 71(1), 106-108.* ([Link](http://onlinelibrary.wiley.com/doi/10.1111/anae.13300/full))

That eureka moment when you look at two columns of figures and the little words at the bottom which say ‘p < 0.05’. All of us have felt this euphoria before. The trial we've spent years working on is a success! We can show that this drug is better than a placebo! Amidst the celebrations and rush to submit the results to a high impact factor journal, not many of us will stop to consider the actual implications of a p value less than 0.05. If asked to give the definition of the p value, it is not often we hear the correct answer. The p value is the probability of finding the observed, or more extreme, results when the null hypothesis of a study question is true. The significance level at which you reject the null hypothesis is arbitrary, and conventionally 5% (less than 1 in 20 chance of being wrong, p < 0.05) is used.

<!--more-->

The p value used in the context of hypothesis testing was popularized by Ronald Fisher in his 1925 book, ‘Statistical methods For research workers’, where he suggested giving 0.05 its special status. In order to understand where the magic 0.05 value comes from, we have to look at the normal distribution curve, shown in Figure 1. On the x-axis (often denoted as z in statistics) is the normal distribution of proportions expressed as standard error of the mean (SEM) distant from the mean proportion (defined as zero). The y-axis can be regarded as ‘how often a proportion will occur’, and so the area under the curve represents 100% of all possible proportions. The SEM distant from the mean proportion value for which p = 0.05, i.e. that which covers 95% of the area under the curve, is 1.96. In other words, 95% of the area under a normal distribution curve lies within 1.96 standard deviations of the mean, and deviations exceeding twice (~1.96) the standard deviation are thus formally regarded as significant.

Since nearly every field of science (with the exception of particle physics) have embraced the use of p < 0.05 as being significant, it is easy to forget that the p value is a continuous variable, and can take on any value between 0 and 1. In addition, the statistical power of the study, seen as ‘how willing we are to make a type-2 error, or accept the null hypothesis when the null hypothesis is false’ has to be taken into consideration. The power of the study is conventionally set at 80%, which means that even if the p value is < 0.05, there is a 20% chance that a type-2 error has been made. Though we dream of having a mathematical model which can give us a definitive answer of whether our results are true or not, the p value cannot give us any information about the qualitative difference between results that fall above, or below this arbitrary binary cut-off. We have to bear in mind that a p value of 0.04 is not greatly different from a p value of 0.06, though we conventionally regard 0.04 as significant and 0.06 as not.

Shortly after Ioannidis’ sensational paper ‘Why most published research findings are false’, criticisms of the use of the p value began to appear in the scientific literature, with suggestions to adopt Bayesian approaches, leading one journal to ban papers using the p value altogether. But it is not the p value itself which is the problem, rather, it is our indiscriminate acceptance that any result with a p value less than 0.05 is worth writing home about which has led to the glut of unreplicable research findings.

There are many reasons why, though we recognize the flaws in relying solely on null-hypothesis testing in science, the p value is here to stay. P values are easy to understand (and easy to calculate) and have the same statistical interpretation whatever the underlying statistic. No matter if it is the geometric mean or the median, if 0.05 is taken as significant, this will always translate to a 5% chance of false positives even if the null hypothesis is true. P values are also useful in multiple testing correction, allowing false discovery rates to be calculated.
