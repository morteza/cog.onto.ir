---
title: 'Why We Need to Do Fewer Statistical Tests'
title_fa:
layout: post_en
date: '2016-09-04'
date_fa:
comments: true
categories: article
link: http://pec.sagepub.com/content/45/5/489
pdf: http://pec.sagepub.com/content/45/5/489.full.pdf+html
doi: 10.1177/0301006616637434
tags:
  - Statistics
  - Frequentists
  - False-Positive Psychology
---

> *Brenner, E. (2016). Why We Need to Do Fewer Statistical Tests. Perception.* ([Link](http://pec.sagepub.com/content/45/5/489))

A recent collaboration, in which a large number of psychological studies were carefully repeated, found that a majority of the findings could not be replicated ([Open Science Collaboration, 2015](http://eprints.lse.ac.uk/65159/1/__lse.ac.uk_storage_LIBRARY_Secondary_libfile_shared_repository_Content_Kappes,%20H_Estimating%20reproducibility_Kappes_Estimating%20the%20reproducibility_2016.pdf)). Is there any reason to believe that articles in *cognitive science journals* are more reliable? If not, is there anything we can do to increase the reliability? In order to answer this question, we need to consider why so many findings could not be replicated. Presumably, this is because the findings were not really true effects in the first place, despite being significant. They were false positives. But why are there so many false positives? Cases of scientists fabricating data to prove their point are disturbing, but not common, so there must be some other reason. Questionable practices such as testing more subjects when an effect is close to significance can certainly make reported statistics less reliable ([Simmons, Nelson, & Simonsohn, 2011](http://pss.sagepub.com/content/22/11/1359)), but this is unlikely to be responsible for the fact that over half the effects are not reproducible.
